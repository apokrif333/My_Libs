{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set desired options\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n",
    "\n",
    "# Add hours features to matrix\n",
    "def add_hours_features(df, X_sparse):\n",
    "    close_hour = (df[times].max(axis=1)).dt.hour\n",
    "    c_morning = ((close_hour >= 7) & (close_hour <= 11)).astype('int')\n",
    "    c_day = ((close_hour >= 12) & (close_hour <= 18)).astype('int')\n",
    "    c_evening = ((close_hour >= 19) & (close_hour <= 23)).astype('int')\n",
    "    \n",
    "    hour = df['time1'].dt.hour\n",
    "    dumm_hour = pd.get_dummies(hour, prefix='h')\n",
    "    \n",
    "    X = hstack([X_sparse,\n",
    "                dumm_hour['h_7'].values.reshape(-1, 1), dumm_hour['h_8'].values.reshape(-1, 1),\n",
    "                dumm_hour['h_9'].values.reshape(-1, 1), dumm_hour['h_10'].values.reshape(-1, 1), dumm_hour['h_11'].values.reshape(-1, 1),\n",
    "                dumm_hour['h_12'].values.reshape(-1, 1), dumm_hour['h_13'].values.reshape(-1, 1), dumm_hour['h_14'].values.reshape(-1, 1),\n",
    "                dumm_hour['h_15'].values.reshape(-1, 1), dumm_hour['h_16'].values.reshape(-1, 1), dumm_hour['h_17'].values.reshape(-1, 1),\n",
    "                dumm_hour['h_18'].values.reshape(-1, 1), dumm_hour['h_19'].values.reshape(-1, 1), dumm_hour['h_20'].values.reshape(-1, 1),\n",
    "                dumm_hour['h_21'].values.reshape(-1, 1), dumm_hour['h_22'].values.reshape(-1, 1), dumm_hour['h_23'].values.reshape(-1, 1),\n",
    "                c_morning.values.reshape(-1, 1), c_day.values.reshape(-1, 1), c_evening.values.reshape(-1, 1)\n",
    "               ])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# Add additional features\n",
    "def add_new_features(df, X_sparse):\n",
    "    start = df[times].min(axis=1)\n",
    "    end = df[times].max(axis=1)\n",
    "    seconds = (end - start) / np.timedelta64(1, 's')\n",
    "    q80_100 = ((seconds > seconds.quantile(0.80)) & (seconds <= seconds.quantile(1.0))).astype(int)\n",
    "    \n",
    "    weekday = df['time1'].dt.weekday\n",
    "    dumm_day = pd.get_dummies(weekday, prefix='d')\n",
    "\n",
    "    X = hstack([X_sparse, q80_100.values.reshape(-1, 1),\n",
    "                dumm_day['d_1'].values.reshape(-1, 1), dumm_day['d_2'].values.reshape(-1, 1), dumm_day['d_3'].values.reshape(-1, 1), dumm_day['d_4'].values.reshape(-1, 1),\n",
    "                dumm_day['d_5'].values.reshape(-1, 1), dumm_day['d_6'].values.reshape(-1, 1),\n",
    "               ])  # alice_macromedia.values.reshape(-1, 1), noalice_728.values.reshape(-1, 1), noalice_780.values.reshape(-1, 1), \n",
    "                        #noalice_778.values.reshape(-1, 1), noalice_570.values.reshape(-1, 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_sessions.csv', index_col='session_id')\n",
    "test_df = pd.read_csv('data/test_sessions.csv', index_col='session_id')\n",
    "\n",
    "# Convert time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[sites].fillna(0).astype('int').to_csv('data/train_sessions_text.txt', sep=' ', \n",
    "                       index=None, header=None)\n",
    "test_df[sites].fillna(0).astype('int').to_csv('data/test_sessions_text.txt', sep=' ', \n",
    "                       index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(ngram_range=(1, 3), max_features=50_000)\n",
    "vectorizer = make_pipeline(cv, TfidfTransformer())\n",
    "\n",
    "with open('data/alice-all/train_sessions_text.txt') as inp_train_file:\n",
    "    X_train = vectorizer.fit_transform(inp_train_file)\n",
    "with open('data/alice-all/test_sessions_text.txt') as inp_test_file:\n",
    "    X_test = vectorizer.transform(inp_test_file)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['target'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "print([(el[0].shape, el[1].shape) for el in time_split.split(X_train)])\n",
    "\n",
    "logit = LogisticRegression(C=1, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[sites] = train_df[sites].fillna(0).astype(np.uint16)\n",
    "test_df[sites] = test_df[sites].fillna(0).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((253561, 50020), (82797, 50020))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаём параметры времени\n",
    "%time\n",
    "X_train_new = add_hours_features(train_df, X_train)\n",
    "X_test_new = add_hours_features(test_df, X_test)\n",
    "X_train_new.shape, X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "X_train_newest = add_new_features(train_df, X_train_new)\n",
    "X_test_newest = add_new_features(test_df, X_test_new)\n",
    "X_train_newest.shape, X_test_newest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for regressor\n",
    "c_values = np.logspace(-2, 2, 10)\n",
    "logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n",
    "                                  scoring='roc_auc', n_jobs=4, cv=time_split, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  3.2min finished\n",
      "c:\\users\\lex\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=17, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'C': array([1.00000e-02, 2.78256e-02, 7.74264e-02, 2.15443e-01, 5.99484e-01,\n",
       "       1.66810e+00, 4.64159e+00, 1.29155e+01, 3.59381e+01, 1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit_grid_searcher.fit(X_train_newest, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9065735045421652, {'C': 4.6415888336127775})"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.best_score_, logit_grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred3 = logit_grid_searcher.predict_proba(X_test_newest)[:, 1]\n",
    "write_to_submission_file(logit_test_pred3, 'data/alice-all/FINAL_subm.csv') # 0.94242"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
